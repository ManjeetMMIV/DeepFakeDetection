{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:16.458611Z",
     "iopub.status.busy": "2025-05-24T06:45:16.458379Z",
     "iopub.status.idle": "2025-05-24T06:45:18.534981Z",
     "shell.execute_reply": "2025-05-24T06:45:18.534176Z",
     "shell.execute_reply.started": "2025-05-24T06:45:16.458587Z"
    },
    "executionInfo": {
     "elapsed": 1627,
     "status": "ok",
     "timestamp": 1747850255079,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "npmPCACts74t",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:18.536047Z",
     "iopub.status.busy": "2025-05-24T06:45:18.535708Z",
     "iopub.status.idle": "2025-05-24T06:45:18.543512Z",
     "shell.execute_reply": "2025-05-24T06:45:18.542755Z",
     "shell.execute_reply.started": "2025-05-24T06:45:18.536026Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1747850255095,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "r-gkIuuCxj7z",
    "outputId": "97bab68d-0451-4579-d0a6-b275cf83bd4c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Adjust the path based on actual folder location\n",
    "fake_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-synthesis'\n",
    "real_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-real'\n",
    "\n",
    "# Check if paths exist\n",
    "print(\"Fake folder exists:\", os.path.exists(fake_folder))\n",
    "print(\"Real folder exists:\", os.path.exists(real_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:18.545522Z",
     "iopub.status.busy": "2025-05-24T06:45:18.545319Z",
     "iopub.status.idle": "2025-05-24T06:45:18.602067Z",
     "shell.execute_reply": "2025-05-24T06:45:18.601313Z",
     "shell.execute_reply.started": "2025-05-24T06:45:18.545506Z"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1747850255143,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "d5L4eXkOxtCq",
    "outputId": "0282742e-b0f0-4a4f-beb5-531226d02165",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "fake_videos = sorted(glob(os.path.join(fake_folder, '*.mp4')))\n",
    "real_videos = sorted(glob(os.path.join(real_folder, '*.mp4')))\n",
    "\n",
    "print(\"Number of fake videos:\", len(fake_videos))\n",
    "print(\"Number of real videos:\", len(real_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:18.603050Z",
     "iopub.status.busy": "2025-05-24T06:45:18.602828Z",
     "iopub.status.idle": "2025-05-24T06:45:18.622644Z",
     "shell.execute_reply": "2025-05-24T06:45:18.621915Z",
     "shell.execute_reply.started": "2025-05-24T06:45:18.603032Z"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1747850255353,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "iV4wh2A5xwJd",
    "outputId": "b1bcf75e-c0a1-49d1-e272-018ce037fa8a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_videos =  real_videos+fake_videos\n",
    "labels = [0]*len(real_videos) + [1]*len(fake_videos)  # 1 = Fake, 0 = Real\n",
    "\n",
    "# Example:\n",
    "for video, label in zip(all_videos, labels):\n",
    "    print(f\"{video} -> {'Fake' if label == 1 else 'Real'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:18.623732Z",
     "iopub.status.busy": "2025-05-24T06:45:18.623520Z",
     "iopub.status.idle": "2025-05-24T06:45:25.740672Z",
     "shell.execute_reply": "2025-05-24T06:45:25.739920Z",
     "shell.execute_reply.started": "2025-05-24T06:45:18.623708Z"
    },
    "executionInfo": {
     "elapsed": 116369,
     "status": "ok",
     "timestamp": 1747850371724,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "b8g-qKLuyEVx",
    "outputId": "7db5e82b-a4d0-4932-aa2a-60da28b3e79c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "frame_count = []\n",
    "for video_file in all_videos:\n",
    "  cap = cv2.VideoCapture(video_file)\n",
    "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "print(\"frames are \" , frame_count)\n",
    "print(\"Total no of video: \" , len(frame_count))\n",
    "print('Average frame per video:',np.mean(frame_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:25.742300Z",
     "iopub.status.busy": "2025-05-24T06:45:25.741510Z",
     "iopub.status.idle": "2025-05-24T06:45:25.745800Z",
     "shell.execute_reply": "2025-05-24T06:45:25.745191Z",
     "shell.execute_reply.started": "2025-05-24T06:45:25.742271Z"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1747850371725,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "lF92C4jay3sS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_video_folder = \"/kaggle/working/output_videos\"\n",
    "os.makedirs(output_video_folder, exist_ok=True)\n",
    "\n",
    "label_file_path = os.path.join(output_video_folder, \"labels.csv\")\n",
    "label_entries = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:25.746663Z",
     "iopub.status.busy": "2025-05-24T06:45:25.746447Z",
     "iopub.status.idle": "2025-05-24T06:45:28.313653Z",
     "shell.execute_reply": "2025-05-24T06:45:28.312942Z",
     "shell.execute_reply.started": "2025-05-24T06:45:25.746647Z"
    },
    "executionInfo": {
     "elapsed": 30445,
     "status": "ok",
     "timestamp": 1747850402161,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "4FxFGGzd0pSt",
    "outputId": "dab3fa5f-a796-42a8-e9ac-307d0b62f209",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install facenet-pytorch --no-deps --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:28.314879Z",
     "iopub.status.busy": "2025-05-24T06:45:28.314621Z",
     "iopub.status.idle": "2025-05-24T06:45:28.323956Z",
     "shell.execute_reply": "2025-05-24T06:45:28.323338Z",
     "shell.execute_reply.started": "2025-05-24T06:45:28.314852Z"
    },
    "executionInfo": {
     "elapsed": 24506,
     "status": "ok",
     "timestamp": 1747850426668,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "Wzs_h4Esy9JJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def filter_and_trim_videos(video_path, output_folder, label, required_frames=150):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    # Only process if video has at least required_frames\n",
    "    if total_frames < required_frames:\n",
    "        cap.release()\n",
    "        print(f\"Skipping {video_name}: only {total_frames} frames (< {required_frames})\")\n",
    "        return None\n",
    "\n",
    "    frames = []\n",
    "    for i in range(required_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < required_frames:\n",
    "        print(f\"Skipping {video_name}: could not read {required_frames} frames\")\n",
    "        return None\n",
    "\n",
    "    # Determine frame size from first frame\n",
    "    if frames:\n",
    "        height, width = frames[0].shape[:2]\n",
    "        size = (width, height)\n",
    "    else:\n",
    "        print(f\"Skipping {video_name}: empty frame list\")\n",
    "        return None\n",
    "\n",
    "    # Save trimmed video\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = os.path.join(output_folder, f\"{video_name}.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 25.0, size)\n",
    "\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Saved trimmed video: {output_path}\")\n",
    "    return video_name, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:45:28.326798Z",
     "iopub.status.busy": "2025-05-24T06:45:28.326280Z",
     "iopub.status.idle": "2025-05-24T06:52:18.876720Z",
     "shell.execute_reply": "2025-05-24T06:52:18.876019Z",
     "shell.execute_reply.started": "2025-05-24T06:45:28.326777Z"
    },
    "id": "Nj54e8cYzAZy",
    "outputId": "bbe2b57e-4b10-4673-df45-e3a7bb1f906f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "fake_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-synthesis'\n",
    "real_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-real'\n",
    "\n",
    "fake_videos = sorted(glob(os.path.join(fake_folder, '*.mp4')))\n",
    "real_videos = sorted(glob(os.path.join(real_folder, '*.mp4')))\n",
    "\n",
    "all_video_paths = [(path, 1) for path in fake_videos] + [(path, 0) for path in real_videos]\n",
    "\n",
    "output_video_folder = '/kaggle/working/trimmed_videos'\n",
    "label_entries = []\n",
    "\n",
    "# --- Make sure output_video_folder exists before the loop ---\n",
    "# This is crucial, as os.path.join won't create the directory.\n",
    "os.makedirs(output_video_folder, exist_ok=True)\n",
    "\n",
    "for path, label in tqdm(all_video_paths):\n",
    "    try:\n",
    "        # filter_and_trim_videos should return the filename (e.g., 'id20_id35_0009.mp4')\n",
    "        # if it returns the full path, you'll need to adjust how you construct full_video_path\n",
    "        result = filter_and_trim_videos(path, output_video_folder, label, required_frames=150)\n",
    "        \n",
    "        if result:\n",
    "            video_filename, processed_label = result # Unpack the result. Renamed `label` to `processed_label` to avoid confusion with loop's `label`\n",
    "            \n",
    "            # Construct the full path to the output video\n",
    "            full_video_path = os.path.join(output_video_folder, video_filename)\n",
    "            \n",
    "            # Append the full path and the label\n",
    "            label_entries.append((full_video_path, processed_label))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing video: {path}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:25.335672Z",
     "iopub.status.busy": "2025-05-24T06:58:25.335400Z",
     "iopub.status.idle": "2025-05-24T06:58:25.343824Z",
     "shell.execute_reply": "2025-05-24T06:58:25.343060Z",
     "shell.execute_reply.started": "2025-05-24T06:58:25.335651Z"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1747850426870,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "wuCEzQ9y39nq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(label_entries, columns=['video_name', 'label'])\n",
    "df_labels.to_csv(label_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:29.276668Z",
     "iopub.status.busy": "2025-05-24T06:58:29.275757Z",
     "iopub.status.idle": "2025-05-24T06:58:29.282121Z",
     "shell.execute_reply": "2025-05-24T06:58:29.281309Z",
     "shell.execute_reply.started": "2025-05-24T06:58:29.276633Z"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1747850427018,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "xC-ySCH14CDc",
    "outputId": "12de7c0e-c37f-4843-c3ca-d634f06af48b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_labels = df_labels.drop_duplicates()\n",
    "df_labels = df_labels.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:32.102191Z",
     "iopub.status.busy": "2025-05-24T06:58:32.101924Z",
     "iopub.status.idle": "2025-05-24T06:58:32.105836Z",
     "shell.execute_reply": "2025-05-24T06:58:32.105084Z",
     "shell.execute_reply.started": "2025-05-24T06:58:32.102173Z"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1747850427038,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "VQZnZ0H74IEY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X=df_labels['video_name']\n",
    "y=df_labels['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:36.293045Z",
     "iopub.status.busy": "2025-05-24T06:58:36.292438Z",
     "iopub.status.idle": "2025-05-24T06:58:36.299209Z",
     "shell.execute_reply": "2025-05-24T06:58:36.298454Z",
     "shell.execute_reply.started": "2025-05-24T06:58:36.293014Z"
    },
    "executionInfo": {
     "elapsed": 3375,
     "status": "error",
     "timestamp": 1747850430414,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "czbQ_6oz4Kev",
    "outputId": "d5762cd6-4c32-4d5f-b975-06d38f8decaa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:38.697372Z",
     "iopub.status.busy": "2025-05-24T06:58:38.697094Z",
     "iopub.status.idle": "2025-05-24T06:58:38.701604Z",
     "shell.execute_reply": "2025-05-24T06:58:38.700902Z",
     "shell.execute_reply.started": "2025-05-24T06:58:38.697351Z"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1747850430419,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "dEb9ECX-4Nm3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:45.284538Z",
     "iopub.status.busy": "2025-05-24T06:58:45.284277Z",
     "iopub.status.idle": "2025-05-24T06:58:45.288790Z",
     "shell.execute_reply": "2025-05-24T06:58:45.288129Z",
     "shell.execute_reply.started": "2025-05-24T06:58:45.284520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train\n",
    "X_train = X_train.apply(lambda x: str(x) + \".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:47.755872Z",
     "iopub.status.busy": "2025-05-24T06:58:47.755587Z",
     "iopub.status.idle": "2025-05-24T06:58:47.762197Z",
     "shell.execute_reply": "2025-05-24T06:58:47.761465Z",
     "shell.execute_reply.started": "2025-05-24T06:58:47.755850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:58:51.675474Z",
     "iopub.status.busy": "2025-05-24T06:58:51.675204Z",
     "iopub.status.idle": "2025-05-24T06:58:51.680605Z",
     "shell.execute_reply": "2025-05-24T06:58:51.679823Z",
     "shell.execute_reply.started": "2025-05-24T06:58:51.675452Z"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "aborted",
     "timestamp": 1747850430420,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "H6j6zoh14UCI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),   # Ensure same spatial size\n",
    "    transforms.ToTensor(),           # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Same normalization as training\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:00:44.950465Z",
     "iopub.status.busy": "2025-05-24T07:00:44.949853Z",
     "iopub.status.idle": "2025-05-24T07:00:44.957308Z",
     "shell.execute_reply": "2025-05-24T07:00:44.956558Z",
     "shell.execute_reply.started": "2025-05-24T07:00:44.950439Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1747850430426,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "qZi__zb34ZSW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_labels['video_name'] = df_labels['video_name'].apply(lambda x: str(x) + \".mp4\")\n",
    "\n",
    "print(\"\\ndf_labels['video_name'] AFTER adding .mp4:\")\n",
    "print(df_labels['video_name'].head())\n",
    "\n",
    "\n",
    "# --- Step 2: Now, create your dictionary using the corrected df_labels ---\n",
    "video_label_dict = dict(zip(df_labels['video_name'], df_labels['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:00:52.099040Z",
     "iopub.status.busy": "2025-05-24T07:00:52.098734Z",
     "iopub.status.idle": "2025-05-24T07:00:52.113499Z",
     "shell.execute_reply": "2025-05-24T07:00:52.112776Z",
     "shell.execute_reply.started": "2025-05-24T07:00:52.099019Z"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "aborted",
     "timestamp": 1747850430427,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "Z5-o-sS14bgD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "video_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T06:52:24.607533Z",
     "iopub.status.busy": "2025-05-24T06:52:24.607359Z",
     "iopub.status.idle": "2025-05-24T06:52:48.252080Z",
     "shell.execute_reply": "2025-05-24T06:52:48.251124Z",
     "shell.execute_reply.started": "2025-05-24T06:52:24.607519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install face_recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:14:24.076379Z",
     "iopub.status.busy": "2025-05-24T07:14:24.076099Z",
     "iopub.status.idle": "2025-05-24T07:14:24.091030Z",
     "shell.execute_reply": "2025-05-24T07:14:24.090331Z",
     "shell.execute_reply.started": "2025-05-24T07:14:24.076359Z"
    },
    "executionInfo": {
     "elapsed": 179550,
     "status": "aborted",
     "timestamp": 1747850430428,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "VTPQN6xa4f5G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VideoDataSet(Dataset):\n",
    "    def __init__(self, video_paths, label_dict, num_frames=10, frame_stride=20, transform=None, device='cpu', debug=False): # device is not used in this class\n",
    "        self.video_paths = video_paths\n",
    "        self.label_dict = label_dict\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_stride = frame_stride\n",
    "        self.transform = transform\n",
    "        self.debug = debug\n",
    "        # self.device = device # device parameter was not used\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.video_paths[index]\n",
    "        \n",
    "        label = self.label_dict.get(path, -1)\n",
    "\n",
    "        frames_tensor_list = self.extract_and_process_frames(path, debug=self.debug, index=index)\n",
    "\n",
    "        # If no frames found, pad with dummy tensors\n",
    "        if len(frames_tensor_list) == 0:\n",
    "            dummy = torch.zeros((3, 112, 112)) # Assuming 3 channels, 112x112 size\n",
    "            frames_tensor_list = [dummy] * self.num_frames\n",
    "\n",
    "        # Pad if fewer than expected\n",
    "        while len(frames_tensor_list) < self.num_frames:\n",
    "            frames_tensor_list.append(frames_tensor_list[-1].clone()) # Pad with the last valid frame\n",
    "\n",
    "        video_tensor = torch.stack(frames_tensor_list)\n",
    "        return video_tensor, label\n",
    "\n",
    "    def extract_and_process_frames(self, path, debug=False, index=0):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {path}\")\n",
    "            return []\n",
    "            \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        processed_frames_tensors = []\n",
    "\n",
    "        for i in range(self.num_frames):\n",
    "            frame_idx = i * self.frame_stride\n",
    "            if frame_idx >= total_frames:\n",
    "                break\n",
    "\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                # print(f\"Warning: Failed to read frame {frame_idx} from video {path}\") # Optional warning\n",
    "                continue\n",
    "\n",
    "            # Convert BGR to RGB\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Detect faces (face_recognition returns list of (top, right, bottom, left))\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "\n",
    "            pil_image_to_transform: Image.Image # This will be the PIL image (face or full frame)\n",
    "\n",
    "            if len(face_locations) == 0:\n",
    "                # No face found, use whole resized frame as fallback\n",
    "                pil_img = Image.fromarray(rgb_frame)\n",
    "                pil_image_to_transform = pil_img.resize((112, 112))\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"[Warning] No face detected at frame {frame_idx} in video index {index} (path: {os.path.basename(path)})\")\n",
    "                    # Plot the image as it was in the original code for this specific \"no face\" debug case:\n",
    "                    # i.e., resized PIL image converted to tensor, before any self.transform.\n",
    "                    temp_tensor_for_plot = transforms.ToTensor()(pil_image_to_transform)\n",
    "                    self.plot_image(temp_tensor_for_plot, index, frame_idx, \"No Face Fallback\")\n",
    "            else:\n",
    "                # Use first detected face\n",
    "                top, right, bottom, left = face_locations[0]\n",
    "                # Ensure coordinates are within image bounds, although face_recognition should handle this.\n",
    "                top = max(0, top)\n",
    "                left = max(0, left)\n",
    "                bottom = min(rgb_frame.shape[0], bottom)\n",
    "                right = min(rgb_frame.shape[1], right)\n",
    "                \n",
    "                face_crop = rgb_frame[top:bottom, left:right]\n",
    "\n",
    "                if face_crop.size == 0: # Check if crop is empty\n",
    "                    # Fallback if face crop is empty for some reason (e.g., invalid coordinates)\n",
    "                    pil_img = Image.fromarray(rgb_frame)\n",
    "                    pil_image_to_transform = pil_img.resize((112, 112))\n",
    "                    if debug:\n",
    "                        print(f\"[Warning] Empty face crop at frame {frame_idx} in video index {index}, using full frame. (path: {os.path.basename(path)})\")\n",
    "                        temp_tensor_for_plot = transforms.ToTensor()(pil_image_to_transform)\n",
    "                        self.plot_image(temp_tensor_for_plot, index, frame_idx, \"Empty Crop Fallback\")\n",
    "                else:\n",
    "                    # Convert to PIL Image and resize\n",
    "                    pil_image_to_transform = Image.fromarray(face_crop).resize((112, 112))\n",
    "\n",
    "            # Apply user-defined transforms (expected to take PIL Image and output Tensor)\n",
    "            # or default ToTensor if no transform is provided.\n",
    "            final_tensor_frame: torch.Tensor\n",
    "            if self.transform:\n",
    "                final_tensor_frame = self.transform(pil_image_to_transform)\n",
    "            else:\n",
    "                final_tensor_frame = transforms.ToTensor()(pil_image_to_transform)\n",
    "\n",
    "            # Visual debug plot for the first frame of the first few samples (shows final processed tensor)\n",
    "            if debug and i == 0 and index < 3:\n",
    "                # This plots the frame after self.transform (or default ToTensor)\n",
    "                # It might replot if \"no face\" debug also plotted, but this shows the *final* tensor.\n",
    "                self.plot_image(final_tensor_frame, index, frame_idx, \"Processed Frame\")\n",
    "\n",
    "            processed_frames_tensors.append(final_tensor_frame)\n",
    "\n",
    "        cap.release()\n",
    "        return processed_frames_tensors\n",
    "\n",
    "    def plot_image(self, image_tensor, video_index, frame_index, title_prefix=\"\"):\n",
    "        # Ensure tensor is on CPU and is a float\n",
    "        npimg = image_tensor.cpu().float().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Normalize for display (handles tensors that are [0,1] or normalized with mean/std)\n",
    "        min_val = npimg.min()\n",
    "        max_val = npimg.max()\n",
    "        if max_val - min_val > 1e-5: # Avoid division by zero/small range\n",
    "            npimg = (npimg - min_val) / (max_val - min_val)\n",
    "        npimg = np.clip(npimg, 0, 1) # Clip to [0,1] range\n",
    "\n",
    "        plt.imshow(npimg)\n",
    "        plt.title(f'{title_prefix} - Video #{video_index + 1} - Frame {frame_index}')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:14:30.071853Z",
     "iopub.status.busy": "2025-05-24T07:14:30.071031Z",
     "iopub.status.idle": "2025-05-24T07:14:31.999167Z",
     "shell.execute_reply": "2025-05-24T07:14:31.998437Z",
     "shell.execute_reply.started": "2025-05-24T07:14:30.071817Z"
    },
    "executionInfo": {
     "elapsed": 179550,
     "status": "aborted",
     "timestamp": 1747850430429,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "HgUIWp5Y4q1A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create your dataset\n",
    "X_train_dataset = VideoDataSet(X_train, video_label_dict, num_frames=10, transform=train_transform, debug=True)\n",
    "\n",
    "# Get the 0th video and its label\n",
    "video_tensor, label = X_train_dataset[65]\n",
    "\n",
    "# Pick a specific frame (e.g., 5th frame, index 4)\n",
    "frame_to_plot = video_tensor[4]\n",
    "\n",
    "# Call the method using the correct dataset object\n",
    "X_train_dataset.plot_image(frame_to_plot, video_index=34, frame_index=9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:14:41.712139Z",
     "iopub.status.busy": "2025-05-24T07:14:41.711842Z",
     "iopub.status.idle": "2025-05-24T07:30:38.694702Z",
     "shell.execute_reply": "2025-05-24T07:30:38.693455Z",
     "shell.execute_reply.started": "2025-05-24T07:14:41.712117Z"
    },
    "executionInfo": {
     "elapsed": 179552,
     "status": "aborted",
     "timestamp": 1747850430433,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "zOTKOq0A4u3o",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(X_train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for videos, labels in train_loader:\n",
    "    print(\"Batch video tensor shape:\", videos.shape)\n",
    "    print(\"Batch labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T06:52:50.244537Z",
     "iopub.status.idle": "2025-05-24T06:52:50.244829Z",
     "shell.execute_reply": "2025-05-24T06:52:50.244715Z",
     "shell.execute_reply.started": "2025-05-24T06:52:50.244701Z"
    },
    "executionInfo": {
     "elapsed": 179552,
     "status": "aborted",
     "timestamp": 1747850430434,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "vHNWnMLx40F6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader=DataLoader(X_test_dataset,batch_size=4,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:31:01.834067Z",
     "iopub.status.busy": "2025-05-24T07:31:01.833577Z",
     "iopub.status.idle": "2025-05-24T07:31:01.841370Z",
     "shell.execute_reply": "2025-05-24T07:31:01.840680Z",
     "shell.execute_reply.started": "2025-05-24T07:31:01.834042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class DeepfakeModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=True):\n",
    "        super(DeepfakeModel, self).__init__()\n",
    "\n",
    "        # Load pretrained ResNeXt50 and remove last two layers\n",
    "        model = models.resnext50_32x4d(pretrained=True)                                                       #Loads a pre-trained ResNeXt-50 model\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])                                              #Extracts all layers except the last two (typically the classification head) to use as a feature extractor.\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        self.relu = nn.LeakyReLU()                                                                            #Initializes a Leaky ReLU activation function\n",
    "        self.dp = nn.Dropout(0.4)                                                                             #Adds a dropout layer with a rate of 0.4 to help prevent overfitting by randomly zeroing out 40% of the neurons during training.\n",
    "        self.linear1 = nn.Linear(2048, num_classes)                                                           #Defines a fully connected layer that maps the LSTM output to the number of classes (2 classes: fake and real).\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape                                                             #c=no. of channels (3-RGB) (h*w=112*112)\n",
    "        x = x.view(batch_size * seq_length, c, h, w)                                                          #Reshapes the input tensor x to (batch_size * seq_length, channels, height, width), allowing each frame to be processed individually by the CNN.\n",
    "        fmap = self.model(x)                                                                                  #Passes the reshaped input through the ResNeXt model to extract feature maps.\n",
    "        x = self.avgpool(fmap)                                                                                #Applies adaptive average pooling to reduce spatial dimensions to 1x1.\n",
    "        x = x.view(batch_size, seq_length, 2048)                                                              #Reshapes the pooled output to (batch_size, seq_length, 2048), making it suitable for input to the LSTM.\n",
    "        x_lstm, _ = self.lstm(x, None)                                                                        #Passes the input tensor through the LSTM layer, initializing the hidden state to zeros; 'x_lstm' contains the LSTM output for each time step and output of hidden state is ignored(_).\n",
    "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1))) \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:31:06.398447Z",
     "iopub.status.busy": "2025-05-24T07:31:06.397726Z",
     "iopub.status.idle": "2025-05-24T07:31:06.405284Z",
     "shell.execute_reply": "2025-05-24T07:31:06.404621Z",
     "shell.execute_reply.started": "2025-05-24T07:31:06.398423Z"
    },
    "executionInfo": {
     "elapsed": 179550,
     "status": "aborted",
     "timestamp": 1747850430436,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "z1QzV9WB4_nR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, optimizer, device):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    class_weights = torch.tensor([1.0, 5.0]).cuda()  # Defines class weights to handle class imbalance\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).cuda()  # Uses cross-entropy loss with class weights\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, (videos, labels) in enumerate(train_loader):\n",
    "            videos = videos.to(device)          # [B, T, 3, H, W]\n",
    "            labels = labels.to(device).float()  # [B], binary (0 or 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through full model (ResNeXt + LSTM)\n",
    "            _, outputs = model(videos)          # outputs: [B]\n",
    "\n",
    "            \n",
    "            labels = labels.to(device).long()\n",
    "            loss = criterion(outputs, labels)   # loss between logits and target\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # Convert logits to probabilities with sigmoid\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)  # shape: [B]\n",
    "\n",
    "\n",
    "            batch_correct = (preds == labels).sum().item()\n",
    "            total_correct += batch_correct\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_accuracy = batch_correct / labels.size(0)\n",
    "            running_accuracy = total_correct / total_samples\n",
    "\n",
    "            print(f\"Batch {batch_idx+1}: Loss={loss.item():.4f}, \"\n",
    "                  f\"Batch Acc={batch_accuracy*100:.2f}%, \"\n",
    "                  f\"Running Acc={running_accuracy*100:.2f}%\")\n",
    "\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        final_accuracy = total_correct / total_samples\n",
    "        print(f\"Epoch {epoch+1} Summary: Avg Loss={avg_loss:.4f}, \"\n",
    "              f\"Accuracy={final_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T07:31:10.844419Z",
     "iopub.status.busy": "2025-05-24T07:31:10.843716Z",
     "iopub.status.idle": "2025-05-24T10:46:41.888036Z",
     "shell.execute_reply": "2025-05-24T10:46:41.887129Z",
     "shell.execute_reply.started": "2025-05-24T07:31:10.844393Z"
    },
    "executionInfo": {
     "elapsed": 179566,
     "status": "aborted",
     "timestamp": 1747850430453,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "PgUosPHS5HXi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DeepfakeModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-5)\n",
    "\n",
    "# assuming train_loader is ready with (videos, labels)\n",
    "\n",
    "train_model(model, train_loader, num_epochs=10, optimizer=optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:48:38.470402Z",
     "iopub.status.busy": "2025-05-24T10:48:38.470121Z",
     "iopub.status.idle": "2025-05-24T10:48:38.899200Z",
     "shell.execute_reply": "2025-05-24T10:48:38.898632Z",
     "shell.execute_reply.started": "2025-05-24T10:48:38.470383Z"
    },
    "executionInfo": {
     "elapsed": 179565,
     "status": "aborted",
     "timestamp": 1747850430454,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "ikYEthdwBe50",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:51:01.893938Z",
     "iopub.status.busy": "2025-05-24T10:51:01.893625Z",
     "iopub.status.idle": "2025-05-24T12:28:30.967353Z",
     "shell.execute_reply": "2025-05-24T12:28:30.966544Z",
     "shell.execute_reply.started": "2025-05-24T10:51:01.893916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_model(model, train_loader, num_epochs=5, optimizer=optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:31:46.885817Z",
     "iopub.status.busy": "2025-05-24T12:31:46.885535Z",
     "iopub.status.idle": "2025-05-24T12:31:47.527459Z",
     "shell.execute_reply": "2025-05-24T12:31:47.526681Z",
     "shell.execute_reply.started": "2025-05-24T12:31:46.885797Z"
    },
    "executionInfo": {
     "elapsed": 179565,
     "status": "aborted",
     "timestamp": 1747850430455,
     "user": {
      "displayName": "Vedant Singh",
      "userId": "09119702006742817659"
     },
     "user_tz": -330
    },
    "id": "mKVerLOPCM6v",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:33:32.367335Z",
     "iopub.status.busy": "2025-05-24T12:33:32.367079Z",
     "iopub.status.idle": "2025-05-24T12:33:32.371346Z",
     "shell.execute_reply": "2025-05-24T12:33:32.370704Z",
     "shell.execute_reply.started": "2025-05-24T12:33:32.367317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.apply(lambda x: str(x) + \".mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:33:39.714938Z",
     "iopub.status.busy": "2025-05-24T12:33:39.714646Z",
     "iopub.status.idle": "2025-05-24T12:33:39.720753Z",
     "shell.execute_reply": "2025-05-24T12:33:39.720232Z",
     "shell.execute_reply.started": "2025-05-24T12:33:39.714905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:34:38.219933Z",
     "iopub.status.busy": "2025-05-24T12:34:38.219634Z",
     "iopub.status.idle": "2025-05-24T12:34:38.223673Z",
     "shell.execute_reply": "2025-05-24T12:34:38.222938Z",
     "shell.execute_reply.started": "2025-05-24T12:34:38.219880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test_dataset = VideoDataSet(X_test, video_label_dict, num_frames=10, transform=train_transform, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:35:30.830155Z",
     "iopub.status.busy": "2025-05-24T12:35:30.829622Z",
     "iopub.status.idle": "2025-05-24T12:35:30.833601Z",
     "shell.execute_reply": "2025-05-24T12:35:30.832871Z",
     "shell.execute_reply.started": "2025-05-24T12:35:30.830131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(X_test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:37:04.944959Z",
     "iopub.status.busy": "2025-05-24T12:37:04.944660Z",
     "iopub.status.idle": "2025-05-24T12:37:24.860908Z",
     "shell.execute_reply": "2025-05-24T12:37:24.860229Z",
     "shell.execute_reply.started": "2025-05-24T12:37:04.944937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i, (videos, labels) in enumerate(test_loader):\n",
    "    print(f\"\\nBatch {i+1}:\")\n",
    "    print(\"Batch video tensor shape:\", videos.shape)  # e.g. torch.Size([4, 3, 16, 64, 64])\n",
    "    print(\"Batch labels:\", labels)\n",
    "\n",
    "    if i == 2:  # Stop after printing 3 batches (index 0, 1, 2)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:40:38.506698Z",
     "iopub.status.busy": "2025-05-24T12:40:38.506415Z",
     "iopub.status.idle": "2025-05-24T12:40:38.833400Z",
     "shell.execute_reply": "2025-05-24T12:40:38.832844Z",
     "shell.execute_reply.started": "2025-05-24T12:40:38.506676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_weights = torch.tensor([1.0, 5.0]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (videos, labels) in enumerate(test_loader):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device).long()\n",
    "\n",
    "            _, outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            batch_correct = (preds == labels).sum().item()\n",
    "            total_correct += batch_correct\n",
    "            total_samples += labels.size(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            batch_accuracy = batch_correct / labels.size(0)\n",
    "            running_accuracy = total_correct / total_samples\n",
    "\n",
    "            print(f\"Test Batch {batch_idx+1}: Loss={loss.item():.4f}, \"\n",
    "                  f\"Batch Acc={batch_accuracy*100:.2f}%, \"\n",
    "                  f\"Running Acc={running_accuracy*100:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / (batch_idx + 1)\n",
    "    final_accuracy = total_correct / total_samples\n",
    "    print(f\"\\nTest Summary: Avg Loss={avg_loss:.4f}, Accuracy={final_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T12:40:45.575211Z",
     "iopub.status.busy": "2025-05-24T12:40:45.574801Z",
     "iopub.status.idle": "2025-05-24T12:45:38.453708Z",
     "shell.execute_reply": "2025-05-24T12:45:38.453062Z",
     "shell.execute_reply.started": "2025-05-24T12:40:45.575188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_model(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOaibDF688dHvjts7iO0/w1",
   "gpuType": "T4",
   "mount_file_id": "16OYVfhAgF3q1BDLZHrPkfIi8BNASea7d",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7480505,
     "sourceId": 11899982,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7491452,
     "sourceId": 11916579,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
