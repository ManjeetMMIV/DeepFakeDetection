{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOaibDF688dHvjts7iO0/w1","gpuType":"T4","mount_file_id":"16OYVfhAgF3q1BDLZHrPkfIi8BNASea7d","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11916579,"sourceType":"datasetVersion","datasetId":7491452},{"sourceId":12196316,"sourceType":"datasetVersion","datasetId":7682569}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:29.078514Z","iopub.execute_input":"2025-09-09T15:37:29.078806Z","iopub.status.idle":"2025-09-09T15:37:29.083193Z","shell.execute_reply.started":"2025-09-09T15:37:29.078779Z","shell.execute_reply":"2025-09-09T15:37:29.082385Z"},"executionInfo":{"elapsed":1627,"status":"ok","timestamp":1747850255079,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"npmPCACts74t","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Adjust the path based on actual folder location\nfake_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-synthesis'\nreal_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-real'\n\n# Check if paths exist\nprint(\"Fake folder exists:\", os.path.exists(fake_folder))\nprint(\"Real folder exists:\", os.path.exists(real_folder))","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:29.084688Z","iopub.execute_input":"2025-09-09T15:37:29.085381Z","iopub.status.idle":"2025-09-09T15:37:29.11202Z","shell.execute_reply.started":"2025-09-09T15:37:29.085363Z","shell.execute_reply":"2025-09-09T15:37:29.111349Z"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1747850255095,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"r-gkIuuCxj7z","outputId":"97bab68d-0451-4579-d0a6-b275cf83bd4c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from glob import glob\n\nfake_videos = sorted(glob(os.path.join(fake_folder, '*.mp4')))\nreal_videos = sorted(glob(os.path.join(real_folder, '*.mp4')))\n\nprint(\"Number of fake videos:\", len(fake_videos))\nprint(\"Number of real videos:\", len(real_videos))","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:29.11277Z","iopub.execute_input":"2025-09-09T15:37:29.113011Z","iopub.status.idle":"2025-09-09T15:37:29.144731Z","shell.execute_reply.started":"2025-09-09T15:37:29.112989Z","shell.execute_reply":"2025-09-09T15:37:29.144162Z"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1747850255143,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"d5L4eXkOxtCq","outputId":"0282742e-b0f0-4a4f-beb5-531226d02165","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_videos =  real_videos+fake_videos\nlabels = [0]*len(real_videos) + [1]*len(fake_videos)  # 1 = Fake, 0 = Real\n\n# Example:\nfor video, label in zip(all_videos, labels):\n    print(f\"{video} -> {'Fake' if label == 1 else 'Real'}\")","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:29.145903Z","iopub.execute_input":"2025-09-09T15:37:29.146086Z","iopub.status.idle":"2025-09-09T15:37:29.16659Z","shell.execute_reply.started":"2025-09-09T15:37:29.146071Z","shell.execute_reply":"2025-09-09T15:37:29.165892Z"},"executionInfo":{"elapsed":172,"status":"ok","timestamp":1747850255353,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"iV4wh2A5xwJd","outputId":"b1bcf75e-c0a1-49d1-e272-018ce037fa8a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frame_count = []\nfor video_file in all_videos:\n  cap = cv2.VideoCapture(video_file)\n  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\nprint(\"frames are \" , frame_count)\nprint(\"Total no of video: \" , len(frame_count))\nprint('Average frame per video:',np.mean(frame_count))","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:29.167377Z","iopub.execute_input":"2025-09-09T15:37:29.168049Z","iopub.status.idle":"2025-09-09T15:37:40.655733Z","shell.execute_reply.started":"2025-09-09T15:37:29.168024Z","shell.execute_reply":"2025-09-09T15:37:40.654844Z"},"executionInfo":{"elapsed":116369,"status":"ok","timestamp":1747850371724,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"b8g-qKLuyEVx","outputId":"7db5e82b-a4d0-4932-aa2a-60da28b3e79c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_video_folder = \"/kaggle/working/output_videos\"\nos.makedirs(output_video_folder, exist_ok=True)\n\nlabel_file_path = os.path.join(output_video_folder, \"labels.csv\")\nlabel_entries = []\n","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:40.656438Z","iopub.execute_input":"2025-09-09T15:37:40.65666Z","iopub.status.idle":"2025-09-09T15:37:40.661017Z","shell.execute_reply.started":"2025-09-09T15:37:40.656643Z","shell.execute_reply":"2025-09-09T15:37:40.660348Z"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1747850371725,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"lF92C4jay3sS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install facenet-pytorch --no-deps --quiet\n\n","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:40.661775Z","iopub.execute_input":"2025-09-09T15:37:40.662029Z","iopub.status.idle":"2025-09-09T15:37:44.936422Z","shell.execute_reply.started":"2025-09-09T15:37:40.662012Z","shell.execute_reply":"2025-09-09T15:37:44.935645Z"},"executionInfo":{"elapsed":30445,"status":"ok","timestamp":1747850402161,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"4FxFGGzd0pSt","outputId":"dab3fa5f-a796-42a8-e9ac-307d0b62f209","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\n\ndef filter_and_trim_videos(video_path, output_folder, label, required_frames=150):\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    video_name = os.path.splitext(os.path.basename(video_path))[0]\n\n    # Only process if video has at least required_frames\n    if total_frames < required_frames:\n        cap.release()\n        print(f\"Skipping {video_name}: only {total_frames} frames (< {required_frames})\")\n        return None\n\n    frames = []\n    for i in range(required_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n\n    cap.release()\n\n    if len(frames) < required_frames:\n        print(f\"Skipping {video_name}: could not read {required_frames} frames\")\n        return None\n\n    # Determine frame size from first frame\n    if frames:\n        height, width = frames[0].shape[:2]\n        size = (width, height)\n    else:\n        print(f\"Skipping {video_name}: empty frame list\")\n        return None\n\n    # Save trimmed video\n    os.makedirs(output_folder, exist_ok=True)\n    output_path = os.path.join(output_folder, f\"{video_name}.mp4\")\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 25.0, size)\n\n    for frame in frames:\n        out.write(frame)\n    out.release()\n\n    print(f\"Saved trimmed video: {output_path}\")\n    return video_name, label\n","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:44.93751Z","iopub.execute_input":"2025-09-09T15:37:44.937787Z","iopub.status.idle":"2025-09-09T15:37:44.945946Z","shell.execute_reply.started":"2025-09-09T15:37:44.937753Z","shell.execute_reply":"2025-09-09T15:37:44.9453Z"},"executionInfo":{"elapsed":24506,"status":"ok","timestamp":1747850426668,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"Wzs_h4Esy9JJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport cv2\nimport os\nfrom glob import glob\nimport os\nfrom tqdm import tqdm\n\nfake_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-synthesis'\nreal_folder = '/kaggle/input/final-dataset/final_dataset/Celeb-real'\n\nfake_videos = sorted(glob(os.path.join(fake_folder, '*.mp4')))\nreal_videos = sorted(glob(os.path.join(real_folder, '*.mp4')))\n\nall_video_paths = [(path, 1) for path in fake_videos] + [(path, 0) for path in real_videos]\n\noutput_video_folder = '/kaggle/working/trimmed_videos'\nlabel_entries = []\n\n# --- Make sure output_video_folder exists before the loop ---\n# This is crucial, as os.path.join won't create the directory.\nos.makedirs(output_video_folder, exist_ok=True)\n\nfor path, label in tqdm(all_video_paths):\n    try:\n        # filter_and_trim_videos should return the filename (e.g., 'id20_id35_0009.mp4')\n        # if it returns the full path, you'll need to adjust how you construct full_video_path\n        result = filter_and_trim_videos(path, output_video_folder, label, required_frames=150)\n        \n        if result:\n            video_filename, processed_label = result # Unpack the result. Renamed `label` to `processed_label` to avoid confusion with loop's `label`\n            \n            # Construct the full path to the output video\n            full_video_path = os.path.join(output_video_folder, video_filename)\n            \n            # Append the full path and the label\n            label_entries.append((full_video_path, processed_label))\n            \n    except Exception as e:\n        print(f\"\\nError processing video: {path}\")\n        print(f\"Error details: {e}\")\n        continue\n","metadata":{"execution":{"iopub.status.busy":"2025-09-09T15:37:44.948159Z","iopub.execute_input":"2025-09-09T15:37:44.94833Z","iopub.status.idle":"2025-09-09T15:44:37.408987Z","shell.execute_reply.started":"2025-09-09T15:37:44.948317Z","shell.execute_reply":"2025-09-09T15:44:37.408169Z"},"id":"Nj54e8cYzAZy","outputId":"bbe2b57e-4b10-4673-df45-e3a7bb1f906f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_labels = pd.DataFrame(label_entries, columns=['video_name', 'label'])\ndf_labels.to_csv(label_file_path, index=False)","metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1747850426870,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"wuCEzQ9y39nq","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:37.409924Z","iopub.execute_input":"2025-09-09T15:44:37.410144Z","iopub.status.idle":"2025-09-09T15:44:37.433971Z","shell.execute_reply.started":"2025-09-09T15:44:37.410127Z","shell.execute_reply":"2025-09-09T15:44:37.433407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_labels = df_labels.drop_duplicates()\ndf_labels = df_labels.reset_index(drop=True)\n","metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1747850427018,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"xC-ySCH14CDc","outputId":"12de7c0e-c37f-4843-c3ca-d634f06af48b","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:37.434684Z","iopub.execute_input":"2025-09-09T15:44:37.434918Z","iopub.status.idle":"2025-09-09T15:44:37.464251Z","shell.execute_reply.started":"2025-09-09T15:44:37.4349Z","shell.execute_reply":"2025-09-09T15:44:37.463648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=df_labels['video_name']\ny=df_labels['label']","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1747850427038,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"VQZnZ0H74IEY","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:37.465066Z","iopub.execute_input":"2025-09-09T15:44:37.465291Z","iopub.status.idle":"2025-09-09T15:44:37.468955Z","shell.execute_reply.started":"2025-09-09T15:44:37.465274Z","shell.execute_reply":"2025-09-09T15:44:37.46834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","metadata":{"executionInfo":{"elapsed":3375,"status":"error","timestamp":1747850430414,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"czbQ_6oz4Kev","outputId":"d5762cd6-4c32-4d5f-b975-06d38f8decaa","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:37.469635Z","iopub.execute_input":"2025-09-09T15:44:37.469898Z","iopub.status.idle":"2025-09-09T15:44:38.170353Z","shell.execute_reply.started":"2025-09-09T15:44:37.469874Z","shell.execute_reply":"2025-09-09T15:44:38.169526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)","metadata":{"executionInfo":{"elapsed":27,"status":"aborted","timestamp":1747850430419,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"dEb9ECX-4Nm3","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:38.171319Z","iopub.execute_input":"2025-09-09T15:44:38.171731Z","iopub.status.idle":"2025-09-09T15:44:38.17588Z","shell.execute_reply.started":"2025-09-09T15:44:38.171711Z","shell.execute_reply":"2025-09-09T15:44:38.175179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train\nX_train = X_train.apply(lambda x: str(x) + \".mp4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:38.176865Z","iopub.execute_input":"2025-09-09T15:44:38.17708Z","iopub.status.idle":"2025-09-09T15:44:38.193172Z","shell.execute_reply.started":"2025-09-09T15:44:38.177063Z","shell.execute_reply":"2025-09-09T15:44:38.192641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:38.193833Z","iopub.execute_input":"2025-09-09T15:44:38.194066Z","iopub.status.idle":"2025-09-09T15:44:38.213946Z","shell.execute_reply.started":"2025-09-09T15:44:38.194048Z","shell.execute_reply":"2025-09-09T15:44:38.213141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize((112, 112)),   # Ensure same spatial size\n    transforms.ToTensor(),           # Convert image to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Same normalization as training\n                         std=[0.229, 0.224, 0.225])\n])\n","metadata":{"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1747850430420,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"H6j6zoh14UCI","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:38.214804Z","iopub.execute_input":"2025-09-09T15:44:38.215043Z","iopub.status.idle":"2025-09-09T15:44:50.079201Z","shell.execute_reply.started":"2025-09-09T15:44:38.215023Z","shell.execute_reply":"2025-09-09T15:44:50.078071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_labels['video_name'] = df_labels['video_name'].apply(lambda x: str(x) + \".mp4\")\n\nprint(\"\\ndf_labels['video_name'] AFTER adding .mp4:\")\nprint(df_labels['video_name'].head())\n\n\n# --- Step 2: Now, create your dictionary using the corrected df_labels ---\nvideo_label_dict = dict(zip(df_labels['video_name'], df_labels['label']))","metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1747850430426,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"qZi__zb34ZSW","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:50.080338Z","iopub.execute_input":"2025-09-09T15:44:50.08096Z","iopub.status.idle":"2025-09-09T15:44:50.090949Z","shell.execute_reply.started":"2025-09-09T15:44:50.080925Z","shell.execute_reply":"2025-09-09T15:44:50.090018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_label_dict","metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1747850430427,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"Z5-o-sS14bgD","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:50.091751Z","iopub.execute_input":"2025-09-09T15:44:50.092056Z","iopub.status.idle":"2025-09-09T15:44:50.124585Z","shell.execute_reply.started":"2025-09-09T15:44:50.092029Z","shell.execute_reply":"2025-09-09T15:44:50.123915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install face_recognition","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:44:50.12532Z","iopub.execute_input":"2025-09-09T15:44:50.125595Z","iopub.status.idle":"2025-09-09T15:45:13.800596Z","shell.execute_reply.started":"2025-09-09T15:44:50.125573Z","shell.execute_reply":"2025-09-09T15:45:13.799824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import face_recognition\nprint(\"face_recognition version:\", face_recognition.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:45:13.801924Z","iopub.execute_input":"2025-09-09T15:45:13.802222Z","iopub.status.idle":"2025-09-09T15:45:15.778165Z","shell.execute_reply.started":"2025-09-09T15:45:13.802188Z","shell.execute_reply":"2025-09-09T15:45:15.777523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport os\nimport numpy as np\nfrom torchvision import transforms\nimport face_recognition\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nclass VideoDataSet(Dataset):\n    def __init__(self, video_paths, label_dict, num_frames=10, frame_stride=20, transform=None, device='cpu', debug=False): # device is not used in this class\n        self.video_paths = video_paths\n        self.label_dict = label_dict\n        self.num_frames = num_frames\n        self.frame_stride = frame_stride\n        self.transform = transform\n        self.debug = debug\n        # self.device = device # device parameter was not used\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, index):\n        path = self.video_paths[index]\n        \n        label = self.label_dict.get(path, -1)\n\n        frames_tensor_list = self.extract_and_process_frames(path, debug=self.debug, index=index)\n\n        # If no frames found, pad with dummy tensors\n        if len(frames_tensor_list) == 0:\n            dummy = torch.zeros((3, 112, 112)) # Assuming 3 channels, 112x112 size\n            frames_tensor_list = [dummy] * self.num_frames\n\n        # Pad if fewer than expected\n        while len(frames_tensor_list) < self.num_frames:\n            frames_tensor_list.append(frames_tensor_list[-1].clone()) # Pad with the last valid frame\n\n        video_tensor = torch.stack(frames_tensor_list)\n        return video_tensor, label\n\n    def extract_and_process_frames(self, path, debug=False, index=0):\n        cap = cv2.VideoCapture(path)\n        if not cap.isOpened():\n            print(f\"Error: Could not open video {path}\")\n            return []\n            \n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        processed_frames_tensors = []\n\n        for i in range(self.num_frames):\n            frame_idx = i * self.frame_stride\n            if frame_idx >= total_frames:\n                break\n\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            success, frame = cap.read()\n            if not success:\n                # print(f\"Warning: Failed to read frame {frame_idx} from video {path}\") # Optional warning\n                continue\n\n            # Convert BGR to RGB\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            # Detect faces (face_recognition returns list of (top, right, bottom, left))\n            face_locations = face_recognition.face_locations(rgb_frame)\n\n            pil_image_to_transform: Image.Image # This will be the PIL image (face or full frame)\n\n            if len(face_locations) == 0:\n                # No face found, use whole resized frame as fallback\n                pil_img = Image.fromarray(rgb_frame)\n                pil_image_to_transform = pil_img.resize((112, 112))\n                \n                if debug:\n                    print(f\"[Warning] No face detected at frame {frame_idx} in video index {index} (path: {os.path.basename(path)})\")\n                    # Plot the image as it was in the original code for this specific \"no face\" debug case:\n                    # i.e., resized PIL image converted to tensor, before any self.transform.\n                    temp_tensor_for_plot = transforms.ToTensor()(pil_image_to_transform)\n                    self.plot_image(temp_tensor_for_plot, index, frame_idx, \"No Face Fallback\")\n            else:\n                # Use first detected face\n                top, right, bottom, left = face_locations[0]\n                # Ensure coordinates are within image bounds, although face_recognition should handle this.\n                top = max(0, top)\n                left = max(0, left)\n                bottom = min(rgb_frame.shape[0], bottom)\n                right = min(rgb_frame.shape[1], right)\n                \n                face_crop = rgb_frame[top:bottom, left:right]\n\n                if face_crop.size == 0: # Check if crop is empty\n                    # Fallback if face crop is empty for some reason (e.g., invalid coordinates)\n                    pil_img = Image.fromarray(rgb_frame)\n                    pil_image_to_transform = pil_img.resize((112, 112))\n                    if debug:\n                        print(f\"[Warning] Empty face crop at frame {frame_idx} in video index {index}, using full frame. (path: {os.path.basename(path)})\")\n                        temp_tensor_for_plot = transforms.ToTensor()(pil_image_to_transform)\n                        self.plot_image(temp_tensor_for_plot, index, frame_idx, \"Empty Crop Fallback\")\n                else:\n                    # Convert to PIL Image and resize\n                    pil_image_to_transform = Image.fromarray(face_crop).resize((112, 112))\n\n            # Apply user-defined transforms (expected to take PIL Image and output Tensor)\n            # or default ToTensor if no transform is provided.\n            final_tensor_frame: torch.Tensor\n            if self.transform:\n                final_tensor_frame = self.transform(pil_image_to_transform)\n            else:\n                final_tensor_frame = transforms.ToTensor()(pil_image_to_transform)\n\n            # Visual debug plot for the first frame of the first few samples (shows final processed tensor)\n            if debug and i == 0 and index < 3:\n                # This plots the frame after self.transform (or default ToTensor)\n                # It might replot if \"no face\" debug also plotted, but this shows the *final* tensor.\n                self.plot_image(final_tensor_frame, index, frame_idx, \"Processed Frame\")\n\n            processed_frames_tensors.append(final_tensor_frame)\n\n        cap.release()\n        return processed_frames_tensors\n\n    def plot_image(self, image_tensor, video_index, frame_index, title_prefix=\"\"):\n        # Ensure tensor is on CPU and is a float\n        npimg = image_tensor.cpu().float().permute(1, 2, 0).numpy()\n        \n        # Normalize for display (handles tensors that are [0,1] or normalized with mean/std)\n        min_val = npimg.min()\n        max_val = npimg.max()\n        if max_val - min_val > 1e-5: # Avoid division by zero/small range\n            npimg = (npimg - min_val) / (max_val - min_val)\n        npimg = np.clip(npimg, 0, 1) # Clip to [0,1] range\n\n        plt.imshow(npimg)\n        plt.title(f'{title_prefix} - Video #{video_index + 1} - Frame {frame_index}')\n        plt.axis('off')\n        plt.show()","metadata":{"executionInfo":{"elapsed":179550,"status":"aborted","timestamp":1747850430428,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"VTPQN6xa4f5G","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:45:15.781389Z","iopub.execute_input":"2025-09-09T15:45:15.781677Z","iopub.status.idle":"2025-09-09T15:45:15.79812Z","shell.execute_reply.started":"2025-09-09T15:45:15.781659Z","shell.execute_reply":"2025-09-09T15:45:15.797388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create your dataset\nX_train_dataset = VideoDataSet(X_train, video_label_dict, num_frames=10, transform=train_transform, debug=True)\n\n# Get the 0th video and its label\nvideo_tensor, label = X_train_dataset[65]\n\n# Pick a specific frame (e.g., 5th frame, index 4)\nframe_to_plot = video_tensor[4]\n\n# Call the method using the correct dataset object\nX_train_dataset.plot_image(frame_to_plot, video_index=34, frame_index=9)\n\n","metadata":{"executionInfo":{"elapsed":179550,"status":"aborted","timestamp":1747850430429,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"HgUIWp5Y4q1A","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:45:15.798957Z","iopub.execute_input":"2025-09-09T15:45:15.799207Z","iopub.status.idle":"2025-09-09T15:45:17.928619Z","shell.execute_reply.started":"2025-09-09T15:45:15.799189Z","shell.execute_reply":"2025-09-09T15:45:17.927927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(X_train_dataset, batch_size=4, shuffle=True)\n\nfor videos, labels in train_loader:\n    print(\"Batch video tensor shape:\", videos.shape)\n    print(\"Batch labels:\", labels)\n","metadata":{"executionInfo":{"elapsed":179552,"status":"aborted","timestamp":1747850430433,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"zOTKOq0A4u3o","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T15:45:17.929307Z","iopub.execute_input":"2025-09-09T15:45:17.929597Z","iopub.status.idle":"2025-09-09T16:04:23.070391Z","shell.execute_reply.started":"2025-09-09T15:45:17.929569Z","shell.execute_reply":"2025-09-09T16:04:23.069733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader=DataLoader(X_test_dataset,batch_size=4,shuffle=True)\n","metadata":{"executionInfo":{"elapsed":179552,"status":"aborted","timestamp":1747850430434,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"vHNWnMLx40F6","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:04:23.071201Z","iopub.execute_input":"2025-09-09T16:04:23.071418Z","iopub.status.idle":"2025-09-09T16:04:23.105601Z","shell.execute_reply.started":"2025-09-09T16:04:23.071401Z","shell.execute_reply":"2025-09-09T16:04:23.104688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass DeepfakeModel(nn.Module):\n    def __init__(self, num_classes=2, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=True):\n        super(DeepfakeModel, self).__init__()\n\n        # Load pretrained ResNeXt50 and remove last two layers\n        model = models.resnext50_32x4d(pretrained=True)                                                       #Loads a pre-trained ResNeXt-50 model\n        self.model = nn.Sequential(*list(model.children())[:-2])                                              #Extracts all layers except the last two (typically the classification head) to use as a feature extractor.\n        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n        self.relu = nn.LeakyReLU()                                                                            #Initializes a Leaky ReLU activation function\n        self.dp = nn.Dropout(0.4)                                                                             #Adds a dropout layer with a rate of 0.4 to help prevent overfitting by randomly zeroing out 40% of the neurons during training.\n        self.linear1 = nn.Linear(2048, num_classes)                                                           #Defines a fully connected layer that maps the LSTM output to the number of classes (2 classes: fake and real).\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        \n    def forward(self, x):\n        batch_size, seq_length, c, h, w = x.shape                                                             #c=no. of channels (3-RGB) (h*w=112*112)\n        x = x.view(batch_size * seq_length, c, h, w)                                                          #Reshapes the input tensor x to (batch_size * seq_length, channels, height, width), allowing each frame to be processed individually by the CNN.\n        fmap = self.model(x)                                                                                  #Passes the reshaped input through the ResNeXt model to extract feature maps.\n        x = self.avgpool(fmap)                                                                                #Applies adaptive average pooling to reduce spatial dimensions to 1x1.\n        x = x.view(batch_size, seq_length, 2048)                                                              #Reshapes the pooled output to (batch_size, seq_length, 2048), making it suitable for input to the LSTM.\n        x_lstm, _ = self.lstm(x, None)                                                                        #Passes the input tensor through the LSTM layer, initializing the hidden state to zeros; 'x_lstm' contains the LSTM output for each time step and output of hidden state is ignored(_).\n        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1))) \n\n\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:05:24.960746Z","iopub.execute_input":"2025-09-09T16:05:24.961034Z","iopub.status.idle":"2025-09-09T16:05:24.969574Z","shell.execute_reply.started":"2025-09-09T16:05:24.961015Z","shell.execute_reply":"2025-09-09T16:05:24.968624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, num_epochs, optimizer, device):\n    model = model.to(device)\n    model.train()\n\n    \n    class_weights = torch.tensor([1.0, 5.0]).cuda()  # Defines class weights to handle class imbalance\n    criterion = nn.CrossEntropyLoss(weight=class_weights).cuda()  # Uses cross-entropy loss with class weights\n\n\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        total_correct = 0\n        total_samples = 0\n\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n\n        for batch_idx, (videos, labels) in enumerate(train_loader):\n            videos = videos.to(device)          # [B, T, 3, H, W]\n            labels = labels.to(device).float()  # [B], binary (0 or 1)\n\n            optimizer.zero_grad()\n\n            # Forward pass through full model (ResNeXt + LSTM)\n            _, outputs = model(videos)          # outputs: [B]\n\n            \n            labels = labels.to(device).long()\n            loss = criterion(outputs, labels)   # loss between logits and target\n            optimizer.zero_grad()\n            loss.backward()\n\n\n\n            optimizer.step()\n\n\n            # Convert logits to probabilities with sigmoid\n            \n            preds = torch.argmax(outputs, dim=1)  # shape: [B]\n\n\n            batch_correct = (preds == labels).sum().item()\n            total_correct += batch_correct\n            total_samples += labels.size(0)\n            total_loss += loss.item()\n\n            batch_accuracy = batch_correct / labels.size(0)\n            running_accuracy = total_correct / total_samples\n\n            print(f\"Batch {batch_idx+1}: Loss={loss.item():.4f}, \"\n                  f\"Batch Acc={batch_accuracy*100:.2f}%, \"\n                  f\"Running Acc={running_accuracy*100:.2f}%\")\n\n        avg_loss = total_loss / (batch_idx + 1)\n        final_accuracy = total_correct / total_samples\n        print(f\"Epoch {epoch+1} Summary: Avg Loss={avg_loss:.4f}, \"\n              f\"Accuracy={final_accuracy*100:.2f}%\")\n","metadata":{"executionInfo":{"elapsed":179550,"status":"aborted","timestamp":1747850430436,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"z1QzV9WB4_nR","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:05:26.57891Z","iopub.execute_input":"2025-09-09T16:05:26.57918Z","iopub.status.idle":"2025-09-09T16:05:26.587326Z","shell.execute_reply.started":"2025-09-09T16:05:26.579161Z","shell.execute_reply":"2025-09-09T16:05:26.586589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = DeepfakeModel().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-5)\n\n# assuming train_loader is ready with (videos, labels)\n\ntrain_model(model, train_loader, num_epochs=10, optimizer=optimizer, device=device)\n","metadata":{"executionInfo":{"elapsed":179566,"status":"aborted","timestamp":1747850430453,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"PgUosPHS5HXi","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T16:05:29.925126Z","iopub.execute_input":"2025-09-09T16:05:29.925376Z","iopub.status.idle":"2025-09-09T19:22:56.165918Z","shell.execute_reply.started":"2025-09-09T16:05:29.925357Z","shell.execute_reply":"2025-09-09T19:22:56.165138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_weights.pth')\n\n","metadata":{"executionInfo":{"elapsed":179565,"status":"aborted","timestamp":1747850430454,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"},"user_tz":-330},"id":"ikYEthdwBe50","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:24:27.985342Z","iopub.execute_input":"2025-09-09T19:24:27.985885Z","iopub.status.idle":"2025-09-09T19:24:28.402552Z","shell.execute_reply.started":"2025-09-09T19:24:27.985862Z","shell.execute_reply":"2025-09-09T19:24:28.401746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = X_test.apply(lambda x: str(x) + \".mp4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:24:59.78192Z","iopub.execute_input":"2025-09-09T19:24:59.782194Z","iopub.status.idle":"2025-09-09T19:24:59.786442Z","shell.execute_reply.started":"2025-09-09T19:24:59.782175Z","shell.execute_reply":"2025-09-09T19:24:59.785713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:25:02.427572Z","iopub.execute_input":"2025-09-09T19:25:02.427834Z","iopub.status.idle":"2025-09-09T19:25:02.434329Z","shell.execute_reply.started":"2025-09-09T19:25:02.427814Z","shell.execute_reply":"2025-09-09T19:25:02.433779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_dataset = VideoDataSet(X_test, video_label_dict, num_frames=10, transform=train_transform, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:25:08.420552Z","iopub.execute_input":"2025-09-09T19:25:08.421228Z","iopub.status.idle":"2025-09-09T19:25:08.424533Z","shell.execute_reply.started":"2025-09-09T19:25:08.421204Z","shell.execute_reply":"2025-09-09T19:25:08.42382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader = DataLoader(X_test_dataset, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:25:12.132507Z","iopub.execute_input":"2025-09-09T19:25:12.133153Z","iopub.status.idle":"2025-09-09T19:25:12.13657Z","shell.execute_reply.started":"2025-09-09T19:25:12.133129Z","shell.execute_reply":"2025-09-09T19:25:12.13588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, (videos, labels) in enumerate(test_loader):\n    print(f\"\\nBatch {i+1}:\")\n    print(\"Batch video tensor shape:\", videos.shape)  # e.g. torch.Size([4, 3, 16, 64, 64])\n    print(\"Batch labels:\", labels)\n\n    if i == 2:  # Stop after printing 3 batches (index 0, 1, 2)\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:25:17.844569Z","iopub.execute_input":"2025-09-09T19:25:17.84483Z","iopub.status.idle":"2025-09-09T19:25:36.771202Z","shell.execute_reply.started":"2025-09-09T19:25:17.844811Z","shell.execute_reply":"2025-09-09T19:25:36.770579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make sure you have this import at the top of your file\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report # MODIFIED IMPORT\n\ndef test_model(model, test_loader, device):\n    model = model.to(device)\n    model.eval()\n\n    # Note: These weights are used for calculating loss, not for evaluation metrics.\n    class_weights = torch.tensor([1.0, 5.0]).to(device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch_idx, (videos, labels) in enumerate(test_loader):\n            videos = videos.to(device)\n            labels = labels.to(device).long()\n\n            _, outputs = model(videos)\n            loss = criterion(outputs, labels)\n\n            preds = torch.argmax(outputs, dim=1)\n\n            # --- This was a duplicate line in your original code, I removed one ---\n            # preds = torch.argmax(outputs, dim=1) \n\n            batch_correct = (preds == labels).sum().item()\n            total_correct += batch_correct\n            total_samples += labels.size(0)\n            total_loss += loss.item()\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            batch_accuracy = batch_correct / labels.size(0)\n            running_accuracy = total_correct / total_samples\n\n            print(f\"Test Batch {batch_idx+1}: Loss={loss.item():.4f}, \"\n                  f\"Batch Acc={batch_accuracy*100:.2f}%, \"\n                  f\"Running Acc={running_accuracy*100:.2f}%\")\n\n    avg_loss = total_loss / (batch_idx + 1)\n    final_accuracy = total_correct / total_samples\n    print(f\"\\nTest Summary: Avg Loss={avg_loss:.4f}, Accuracy={final_accuracy*100:.2f}%\")\n\n    # --- NEW CODE BLOCK STARTS HERE ---\n\n    # Generate and print the classification report\n    # Using target_names makes the report easier to read\n    print(\"\\nClassification Report:\")\n    target_names = ['Real (Class 0)', 'Deepfake (Class 1)']\n    report = classification_report(all_labels, all_preds, target_names=target_names)\n    print(report)\n\n    # --- NEW CODE BLOCK ENDS HERE ---\n\n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(6,5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_model(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:46:20.5993Z","iopub.execute_input":"2025-09-09T19:46:20.599571Z","iopub.status.idle":"2025-09-09T19:51:12.25757Z","shell.execute_reply.started":"2025-09-09T19:46:20.599554Z","shell.execute_reply":"2025-09-09T19:51:12.256953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T19:39:15.47339Z","iopub.execute_input":"2025-09-09T19:39:15.474104Z","iopub.status.idle":"2025-09-09T19:39:16.120329Z","shell.execute_reply.started":"2025-09-09T19:39:15.474084Z","shell.execute_reply":"2025-09-09T19:39:16.119494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}